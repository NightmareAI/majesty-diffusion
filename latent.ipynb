{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/multimodalart/majesty-diffusion/blob/main/latent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUmmV5ZvrPbP"
      },
      "source": [
        "# Latent Majesty Diffusion v1.6\n",
        "#### Formerly known as Princess Generator\n",
        "##### Access our [Majestic Guide](https://multimodal.art/majesty-diffusion) (_under construction_), our [GitHub](https://github.com/multimodalart/majesty-diffusion), join our community on [Discord](https://discord.gg/yNBtQBEDfZ) or reach out via [@multimodalart on Twitter](https://twitter.com/multimodalart))\n",
        "\\\n",
        " \n",
        "---\n",
        "\\\n",
        "\n",
        "\n",
        "#### CLIP Guided Latent Diffusion by [dango233](https://github.com/Dango233/) and [apolinario (@multimodalart)](https://twitter.com/multimodalart). \n",
        "The LAION-400M-trained model and the modified inference code are from [CompVis Latent Diffusion](https://github.com/CompVis/latent-diffusion). The guided-diffusion method is modified by Dango233 based on [Katherine Crowson](https://twitter.com/RiversHaveWings)'s guided diffusion notebook. multimodalart savable settings, MMC and assembled the Colab. Check the complete list on our GitHub. Some functions and methods are from various code masters (nsheppard, DanielRussRuss and others)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOAs3ZvLlktt"
      },
      "source": [
        "## Changelog\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "p15Fm1AjloLa"
      },
      "outputs": [],
      "source": [
        "#@markdown Release: 1.2 (prior versions were Princess Generator and you can check [GitHub out for that](https://github.com/multimodalart/majesty-diffusion/))\n",
        "\n",
        "#@markdown Changelog: 1.3 - better upscaler (learn how to use it on our [Majestic Guide](https://multimodal.art/majesty-diffusion))\n",
        "\n",
        "#@markdown Changelog: 1.4 - better defaults, added OpenCLIP ViT-L/14 LAION-400M, fix CLOOB, adds modified dynamic thresholding, removes latent upscaler (was broken), adds RGB upscaler \n",
        "\n",
        "#@markdown Changelog 1.5 - even better defaults, better dynamic thresholidng, fixes range scale, adds var and mean scales, adds the possibility of blurring cuts\n",
        "\n",
        "#@markdown Changelog 1.6 - ViT-L conditioning for latenet diffusion, adds noising and scaling during advanced scheduling phases, fixes linear ETA, adss LAION models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWLsDt7wkZfU"
      },
      "source": [
        "## Save model and outputs on Google Drive? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aJF6wP2zkWE_"
      },
      "outputs": [],
      "source": [
        "#@markdown Enable saving outputs to Google Drive to save your creations at AI/models\n",
        "save_outputs_to_google_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown Enable saving models to Google Drive to avoid downloading the model every Colab instance\n",
        "save_models_to_google_drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "if save_outputs_to_google_drive or save_models_to_google_drive:\n",
        "    from google.colab import drive\n",
        "    try:\n",
        "      drive.mount('/content/gdrive')\n",
        "    except:\n",
        "      save_outputs_to_google_drive = False\n",
        "      save_models_to_google_drive = False\n",
        "\n",
        "model_path = \"/content/gdrive/MyDrive/AI/models\" if save_models_to_google_drive else \"/content/\"\n",
        "outputs_path = \"/content/gdrive/MyDrive/AI/latent_majesty_diffusion\" if save_outputs_to_google_drive else \"/content/outputs\"\n",
        "!mkdir -p $model_path\n",
        "!mkdir -p $outputs_path\n",
        "print(f\"Model will be stored at {model_path}\")\n",
        "print(f\"Outputs will be saved to {outputs_path}\")\n",
        "\n",
        "#If you want to run it locally change it to true\n",
        "is_local = False\n",
        "skip_installs = False\n",
        "if(is_local):\n",
        "  model_path = \"/choose/your/local/model/path\"\n",
        "  outputs_path = \"/choose/your/local/outputs/path\"\n",
        "  skip_installs = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5Fxt-5TaYBs2"
      },
      "outputs": [],
      "source": [
        "#@title Model settings\n",
        "#@markdown The `original` model is the model trained by CompVis in the LAION-400M dataset\n",
        "#@markdown <br>The `finetuned` model is a finetune of the `original` model [by Jack000](https://github.com/Jack000/glid-3-xl) that generates less watermarks, but is a bit worse in text synthesis. Colab Free does not have enough run for the finetuned (for now)\n",
        "#@markdown <br>The `ongo` and `erlich` models are models [fine-tuned by LAION](https://github.com/LAION-AI/ldm-finetune)on art (ongo) and erlich (logos) \n",
        "latent_diffusion_model = 'finetuned' #@param [\"original\", \"finetuned\", \"ongo (fine tuned in paintings)\", \"erlich (fine tuned in logos)\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEVSOJ4f0B21"
      },
      "source": [
        "# Setup stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NHgUAp48qwoG"
      },
      "outputs": [],
      "source": [
        "#@title Installation\n",
        "if(not skip_installs):\n",
        "    import subprocess\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    cards_requiring_downgrade = [\"Tesla T4\", \"V100\"]\n",
        "    if any(cardstr in nvidiasmi_output for cardstr in cards_requiring_downgrade):\n",
        "        downgrade_pytorch_result = subprocess.run(['pip', 'install', 'torch==1.10.2', 'torchvision==0.11.3', '-q'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    import sys\n",
        "    sys.path.append(\".\")\n",
        "    !git clone https://github.com/multimodalart/latent-diffusion --branch 1.6\n",
        "    !git clone https://github.com/CompVis/taming-transformers\n",
        "    !git clone https://github.com/TencentARC/GFPGAN\n",
        "    !git lfs clone https://huggingface.co/datasets/multimodalart/latent-majesty-diffusion-settings\n",
        "    !git clone https://github.com/NightmareAI/majesty-diffusion --branch 1.6-cli\n",
        "    !git lfs clone https://github.com/LAION-AI/aesthetic-predictor\n",
        "    !pip install -e ./taming-transformers\n",
        "    !pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 torch-fidelity einops\n",
        "    !pip install transformers\n",
        "    !pip install dotmap\n",
        "    !pip install resize-right\n",
        "    !pip install piq\n",
        "    !pip install lpips\n",
        "    !pip install basicsr\n",
        "    !pip install facexlib\n",
        "    !pip install realesrgan\n",
        "\n",
        "    sys.path.append('./taming-transformers')\n",
        "    from taming.models import vqgan\n",
        "    from subprocess import Popen, PIPE\n",
        "    try:\n",
        "        import mmc\n",
        "    except:\n",
        "        # install mmc\n",
        "        !git clone https://github.com/apolinario/Multi-Modal-Comparators --branch gradient_checkpointing\n",
        "        !pip install poetry\n",
        "        !cd Multi-Modal-Comparators; poetry build\n",
        "        !cd Multi-Modal-Comparators; pip install dist/mmc*.whl\n",
        "        \n",
        "        # optional final step:\n",
        "        #poe napm_installs\n",
        "        !python Multi-Modal-Comparators/src/mmc/napm_installs/__init__.py\n",
        "    # suppress mmc warmup outputs\n",
        "    import mmc.loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNqCqQDoyZmq"
      },
      "source": [
        "Now, download the checkpoint (~5.7 GB). This will usually take 3-6 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cNHvQBhzyXCI"
      },
      "outputs": [],
      "source": [
        "#@title Download models\n",
        "import sys\n",
        "sys.path.append(\"./latent-diffusion\")\n",
        "sys.path.append('./majesty-diffusion')\n",
        "import majesty\n",
        "majesty.model_path = model_path\n",
        "ongo = latent_diffusion_model.startswith('ongo')\n",
        "erlich = latent_diffusion_model.startswith('erlich')\n",
        "majesty.download_models(ongo, erlich)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThxmCePqt1mt"
      },
      "source": [
        "Let's also check what type of GPU we've got."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbL2zJ7Pt7Jl"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BPnyd-XUKbfE"
      },
      "outputs": [],
      "source": [
        "#@title Import stuff\n",
        "import argparse, os, sys, glob\n",
        "sys.path.append('./majesty-diffusion')\n",
        "import majesty\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm, trange\n",
        "tqdm_auto_model = __import__(\"tqdm.auto\", fromlist=[None]) \n",
        "sys.modules['tqdm'] = tqdm_auto_model\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import transformers\n",
        "import gc\n",
        "sys.path.append('./latent-diffusion')\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "from ldm.modules.diffusionmodules.util import noise_like, make_ddim_sampling_parameters\n",
        "import tensorflow as tf\n",
        "from dotmap import DotMap\n",
        "import ipywidgets as widgets\n",
        "from math import pi\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "import gc\n",
        "import io\n",
        "import math\n",
        "import sys\n",
        "import random\n",
        "from piq import brisque\n",
        "from itertools import product\n",
        "from IPython import display\n",
        "import lpips\n",
        "from PIL import Image, ImageOps\n",
        "import requests\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from torchvision import transforms as T\n",
        "from torchvision.transforms import functional as TF\n",
        "from numpy import nan\n",
        "from threading import Thread\n",
        "import time\n",
        "import re\n",
        "import base64\n",
        "\n",
        "#sys.path.append('../CLIP')\n",
        "#Resizeright for better gradient when resizing\n",
        "#sys.path.append('../ResizeRight/')\n",
        "#sys.path.append('../cloob-training/')\n",
        "\n",
        "from resize_right import resize\n",
        "\n",
        "import clip\n",
        "#from cloob_training import model_pt, pretrained\n",
        "\n",
        "#pretrained.list_configs()\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twG4nxYCrI8F"
      },
      "outputs": [],
      "source": [
        "#@title Load the model\n",
        "majesty.model_path = model_path\n",
        "majesty.outputs_path = outputs_path\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "majesty.device = device\n",
        "\n",
        "config = OmegaConf.load(\"./latent-diffusion/configs/latent-diffusion/txt2img-1p4B-eval.yaml\")  # TODO: Optionally download from same location as ckpt and chnage this logic\n",
        "model = majesty.load_model_from_config(config, f\"{model_path}/latent_diffusion_txt2img_f8_large.ckpt\",False, latent_diffusion_model)  # TODO: check path\n",
        "majesty.model = model.half().eval().to(device)\n",
        "#if(latent_diffusion_model == \"finetuned\"):\n",
        "#  model.model = model.model.half().eval().to(device)\n",
        "majesty.load_lpips_model()\n",
        "majesty.load_aesthetic_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILHGCEla2Rrm"
      },
      "source": [
        "# Run!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpR9JhyCu5iq"
      },
      "source": [
        "#### Perceptors (Choose your CLIP and CLIP-like models) \n",
        "Be careful if you don't pay for Colab Pro selecting more CLIPs might make you go out of memory. If you do have Pro, try adding ViT-L14 to your mix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8K7l_E2JvLWC"
      },
      "outputs": [],
      "source": [
        "#@title Choose your perceptor models\n",
        "\n",
        "# suppress mmc warmup outputs\n",
        "import mmc.loaders\n",
        "clip_load_list = []\n",
        "#@markdown #### Open AI CLIP models\n",
        "ViT_B32 = False #@param {type:\"boolean\"}\n",
        "ViT_B16 = True #@param {type:\"boolean\"}\n",
        "ViT_L14 = True #@param {type:\"boolean\"}\n",
        "ViT_L14_336px = False #@param {type:\"boolean\"}\n",
        "#RN101 = False #@param {type:\"boolean\"}\n",
        "#RN50 = False #@param {type:\"boolean\"}\n",
        "RN50x4 = False #@param {type:\"boolean\"}\n",
        "RN50x16 = False #@param {type:\"boolean\"}\n",
        "RN50x64 = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown #### OpenCLIP models\n",
        "ViT_B16_plus = False #@param {type: \"boolean\"}\n",
        "ViT_B32_laion2b = True #@param {type: \"boolean\"}\n",
        "ViT_L14_laion = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown #### Multilangual CLIP models \n",
        "clip_farsi = False #@param {type: \"boolean\"}\n",
        "clip_korean = False #@param {type: \"boolean\"}\n",
        "\n",
        "#@markdown #### CLOOB models\n",
        "cloob_ViT_B16 = False #@param {type: \"boolean\"}\n",
        "\n",
        "# @markdown Load even more CLIP and CLIP-like models (from [Multi-Modal-Comparators](https://github.com/dmarx/Multi-Modal-Comparators))\n",
        "model1 = \"\" # @param [\"[clip - mlfoundations - RN50--openai]\",\"[clip - mlfoundations - RN101--openai]\",\"[clip - mlfoundations - RN50--yfcc15m]\",\"[clip - mlfoundations - RN50--cc12m]\",\"[clip - mlfoundations - RN50-quickgelu--yfcc15m]\",\"[clip - mlfoundations - RN50-quickgelu--cc12m]\",\"[clip - mlfoundations - RN101--yfcc15m]\",\"[clip - mlfoundations - RN101-quickgelu--yfcc15m]\",\"[clip - mlfoundations - ViT-B-32--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-32--laion400m_e32]\",\"[clip - mlfoundations - ViT-B-32--laion400m_avg]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_e32]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_avg]\",\"[clip - mlfoundations - ViT-B-16--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-16--laion400m_e32]\",\"[clip - sbert - ViT-B-32-multilingual-v1]\",\"[clip - facebookresearch - clip_small_25ep]\",\"[simclr - facebookresearch - simclr_small_25ep]\",\"[slip - facebookresearch - slip_small_25ep]\",\"[slip - facebookresearch - slip_small_50ep]\",\"[slip - facebookresearch - slip_small_100ep]\",\"[clip - facebookresearch - clip_base_25ep]\",\"[simclr - facebookresearch - simclr_base_25ep]\",\"[slip - facebookresearch - slip_base_25ep]\",\"[slip - facebookresearch - slip_base_50ep]\",\"[slip - facebookresearch - slip_base_100ep]\",\"[clip - facebookresearch - clip_large_25ep]\",\"[simclr - facebookresearch - simclr_large_25ep]\",\"[slip - facebookresearch - slip_large_25ep]\",\"[slip - facebookresearch - slip_large_50ep]\",\"[slip - facebookresearch - slip_large_100ep]\",\"[clip - facebookresearch - clip_base_cc3m_40ep]\",\"[slip - facebookresearch - slip_base_cc3m_40ep]\",\"[slip - facebookresearch - slip_base_cc12m_35ep]\",\"[clip - facebookresearch - clip_base_cc12m_35ep]\"] {allow-input: true}\n",
        "model2 = \"\" # @param [\"[clip - mlfoundations - RN50--openai]\",\"[clip - mlfoundations - RN101--openai]\",\"[clip - mlfoundations - RN50--yfcc15m]\",\"[clip - mlfoundations - RN50--cc12m]\",\"[clip - mlfoundations - RN50-quickgelu--yfcc15m]\",\"[clip - mlfoundations - RN50-quickgelu--cc12m]\",\"[clip - mlfoundations - RN101--yfcc15m]\",\"[clip - mlfoundations - RN101-quickgelu--yfcc15m]\",\"[clip - mlfoundations - ViT-B-32--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-32--laion400m_e32]\",\"[clip - mlfoundations - ViT-B-32--laion400m_avg]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_e32]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_avg]\",\"[clip - mlfoundations - ViT-B-16--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-16--laion400m_e32]\",\"[clip - sbert - ViT-B-32-multilingual-v1]\",\"[clip - facebookresearch - clip_small_25ep]\",\"[simclr - facebookresearch - simclr_small_25ep]\",\"[slip - facebookresearch - slip_small_25ep]\",\"[slip - facebookresearch - slip_small_50ep]\",\"[slip - facebookresearch - slip_small_100ep]\",\"[clip - facebookresearch - clip_base_25ep]\",\"[simclr - facebookresearch - simclr_base_25ep]\",\"[slip - facebookresearch - slip_base_25ep]\",\"[slip - facebookresearch - slip_base_50ep]\",\"[slip - facebookresearch - slip_base_100ep]\",\"[clip - facebookresearch - clip_large_25ep]\",\"[simclr - facebookresearch - simclr_large_25ep]\",\"[slip - facebookresearch - slip_large_25ep]\",\"[slip - facebookresearch - slip_large_50ep]\",\"[slip - facebookresearch - slip_large_100ep]\",\"[clip - facebookresearch - clip_base_cc3m_40ep]\",\"[slip - facebookresearch - slip_base_cc3m_40ep]\",\"[slip - facebookresearch - slip_base_cc12m_35ep]\",\"[clip - facebookresearch - clip_base_cc12m_35ep]\"] {allow-input: true}\n",
        "model3 = \"\" # @param [\"[clip - openai - RN50]\",\"[clip - openai - RN101]\",\"[clip - mlfoundations - RN50--yfcc15m]\",\"[clip - mlfoundations - RN50--cc12m]\",\"[clip - mlfoundations - RN50-quickgelu--yfcc15m]\",\"[clip - mlfoundations - RN50-quickgelu--cc12m]\",\"[clip - mlfoundations - RN101--yfcc15m]\",\"[clip - mlfoundations - RN101-quickgelu--yfcc15m]\",\"[clip - mlfoundations - ViT-B-32--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-32--laion400m_e32]\",\"[clip - mlfoundations - ViT-B-32--laion400m_avg]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_e32]\",\"[clip - mlfoundations - ViT-B-32-quickgelu--laion400m_avg]\",\"[clip - mlfoundations - ViT-B-16--laion400m_e31]\",\"[clip - mlfoundations - ViT-B-16--laion400m_e32]\",\"[clip - sbert - ViT-B-32-multilingual-v1]\",\"[clip - facebookresearch - clip_small_25ep]\",\"[simclr - facebookresearch - simclr_small_25ep]\",\"[slip - facebookresearch - slip_small_25ep]\",\"[slip - facebookresearch - slip_small_50ep]\",\"[slip - facebookresearch - slip_small_100ep]\",\"[clip - facebookresearch - clip_base_25ep]\",\"[simclr - facebookresearch - simclr_base_25ep]\",\"[slip - facebookresearch - slip_base_25ep]\",\"[slip - facebookresearch - slip_base_50ep]\",\"[slip - facebookresearch - slip_base_100ep]\",\"[clip - facebookresearch - clip_large_25ep]\",\"[simclr - facebookresearch - simclr_large_25ep]\",\"[slip - facebookresearch - slip_large_25ep]\",\"[slip - facebookresearch - slip_large_50ep]\",\"[slip - facebookresearch - slip_large_100ep]\",\"[clip - facebookresearch - clip_base_cc3m_40ep]\",\"[slip - facebookresearch - slip_base_cc3m_40ep]\",\"[slip - facebookresearch - slip_base_cc12m_35ep]\",\"[clip - facebookresearch - clip_base_cc12m_35ep]\"] {allow-input: true}\n",
        "\n",
        "if ViT_B32: \n",
        "  clip_load_list.append(\"[clip - mlfoundations - ViT-B-32--openai]\")\n",
        "if ViT_B16: \n",
        "  clip_load_list.append(\"[clip - mlfoundations - ViT-B-16--openai]\")\n",
        "if ViT_L14: \n",
        "  clip_load_list.append(\"[clip - mlfoundations - ViT-L-14--openai]\")\n",
        "if RN50x4: \n",
        "  clip_load_list.append(\"[clip - mlfoundations - RN50x4--openai]\")\n",
        "if RN50x64: \n",
        "  clip_load_list.append(\"[clip - mlfoundations - RN50x64--openai]\")\n",
        "if RN50x16: \n",
        "  clip_load_list.append(\"[clip - mlfoundations - RN50x16--openai]\")\n",
        "if ViT_L14_laion: \n",
        "  clip_load_list.append(\"[clip - mlfoundations - ViT-L-14--laion400m_e32]\")\n",
        "if ViT_L14_336px:\n",
        "  clip_load_list.append(\"[clip - mlfoundations - ViT-L-14-336--openai]\")\n",
        "if ViT_B16_plus:\n",
        "  clip_load_list.append(\"[clip - mlfoundations - ViT-B-16-plus-240--laion400m_e32]\")\n",
        "if ViT_B32_laion2b:\n",
        "  clip_load_list.append(\"[clip - mlfoundations - ViT-B-32--laion2b_e16]\")\n",
        "if clip_farsi:\n",
        "  clip_load_list.append(\"[clip - sajjjadayobi - clipfa]\")\n",
        "if clip_korean:\n",
        "  clip_load_list.append(\"[clip - navervision - kelip_ViT-B/32]\")\n",
        "if cloob_ViT_B16:\n",
        "  clip_load_list.append(\"[cloob - crowsonkb - cloob_laion_400m_vit_b_16_32_epochs]\")\n",
        "\n",
        "if model1:\n",
        "  clip_load_list.append(model1)\n",
        "if model2:\n",
        "  clip_load_list.append(model2)\n",
        "if model3:\n",
        "  clip_load_list.append(model3)\n",
        "\n",
        "\n",
        "majesty.clip_load_list = clip_load_list\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Di3xFSXGWe"
      },
      "source": [
        "#### Advanced settings for the generation\n",
        "##### Access [our guide](https://multimodal.art/majesty-diffusion)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAALegoCXEbm"
      },
      "outputs": [],
      "source": [
        "opt = DotMap()\n",
        "\n",
        "#Change it to false to not use CLIP Guidance at all \n",
        "majesty.use_cond_fn = True\n",
        "\n",
        "#Custom cut schedules and super-resolution. Check out the guide on how to use it a https://multimodal.art/majestydiffusion\n",
        "majesty.custom_schedule_setting = [\n",
        " [50,1000,8],\n",
        " \"gfpgan:1.5\",\"scale:.9\",\"noise:.55\",\n",
        " [50,200,5],\n",
        "]\n",
        "              \n",
        "#Cut settings\n",
        "majesty.clamp_index = [2.1,1.6] #linear variation of the index for clamping the gradient  \n",
        "majesty.cut_overview = [8]*500 + [4]*500\n",
        "majesty.cut_innercut = [0]*500 + [4]*500\n",
        "majesty.cut_ic_pow = .2\n",
        "majesty.cut_icgray_p = [.1]*300+[0]*1000\n",
        "majesty.cutn_batches = 1\n",
        "majesty.cut_blur_n = [0]*300 + [0]*1000\n",
        "majesty.cut_blur_kernel = 3\n",
        "majesty.range_index =  [0]*200+ [5e4]*400 + [0]*1000\n",
        "majesty.var_index = [2]*300+[0]*700\n",
        "majesty.var_range = 0.5\n",
        "majesty.mean_index = [0]*400+[0]*600\n",
        "majesty.mean_range = 0.75\n",
        "majesty.active_function = \"softsign\" # function to manipulate the gradient - help things to stablize\n",
        "majesty.ths_method = \"clamp\"\n",
        "majesty.tv_scales = [600]*1+[50]*1 +[0]*2\n",
        "\n",
        "#If you uncomment next line you can schedule the CLIP guidance across the steps. Otherwise the clip_guidance_scale basic setting will be used\n",
        "#clip_guidance_schedule = [10000]*300 + [500]*700\n",
        "\n",
        "majesty.symmetric_loss_scale = 0 #Apply symmetric loss\n",
        "\n",
        "#Latent Diffusion Advanced Settings\n",
        "majesty.scale_div = 1 # Use when latent upscale to correct satuation problem\n",
        "majesty.opt_mag_mul = 20 #Magnify grad before clamping\n",
        "#PLMS Currently not working, working on a fix\n",
        "opt.plms = False #Experimental. It works but does not lookg good\n",
        "majesty.opt_ddim_eta, majesty.opt_eta_end = [1.3,1.1] # linear variation of eta\n",
        "majesty.opt_temperature = .98\n",
        "\n",
        "#Grad advanced settings\n",
        "majesty.grad_center = False\n",
        "majesty.grad_scale= 0.25 #Lower value result in more coherent and detailed result, higher value makes it focus on more dominent concept\n",
        "\n",
        "#Restraints the model from explodign despite larger clamp\n",
        "majesty.score_modifier = True\n",
        "majesty.threshold_percentile = .85\n",
        "majesty.threshold = 1\n",
        "majesty.score_corrector_setting = [\"latent\", \"\"]\n",
        "\n",
        "#Init image advanced settings\n",
        "majesty.init_rotate, majesty.mask_rotate=[False, False]\n",
        "majesty.init_magnitude = 0.18215\n",
        "\n",
        "#More settings\n",
        "majesty.RGB_min, majesty.RGB_max = [-0.95,0.95]\n",
        "majesty.padargs = {\"mode\":\"constant\", \"value\": -1} #How to pad the image with cut_overview\n",
        "majesty.flip_aug=False\n",
        "majesty.cutout_debug = False\n",
        "majesty.opt.outdir = outputs_path\n",
        "\n",
        "#Experimental aesthetic embeddings, work only with OpenAI ViT-B/32 and ViT-L/14\n",
        "majesty.experimental_aesthetic_embeddings = True\n",
        "#How much you want this to influence your result\n",
        "majesty.experimental_aesthetic_embeddings_weight = 0.3\n",
        "#9 are good aesthetic embeddings, 0 are bad ones\n",
        "majesty.experimental_aesthetic_embeddings_score = 8\n",
        "\n",
        "# For fun dont change except if you really know what your are doing\n",
        "majesty.grad_blur = False\n",
        "majesty.compress_steps = 200\n",
        "majesty.compress_factor = 0.1\n",
        "majesty.punish_steps = 200\n",
        "majesty.punish_factor = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUu_pyTkuxiT"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo1tM270ryit"
      },
      "source": [
        "### Prompts\n",
        "The main prompt is the CLIP prompt. The Latent Prompts usually help with style and composition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRIC0eQervDN"
      },
      "outputs": [],
      "source": [
        "#Amp up your prompt game with prompt engineering, check out this guide: https://matthewmcateer.me/blog/clip-prompt-engineering/\n",
        "#Prompt for CLIP Guidance\n",
        "clip_prompts =[\"The portrait of a Majestic Princess, trending on artstation\"]  \n",
        "\n",
        "#Prompt for Latent Diffusion\n",
        "latent_prompts = [\"The portrait of a Majestic Princess, trending on artstation\"]  \n",
        "\n",
        "#Negative prompts for Latent Diffusion\n",
        "latent_negatives = [\"\"]\n",
        "\n",
        "majesty.image_prompts = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv8-gEvUsADL"
      },
      "source": [
        "### Diffuse!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fmafGmcyT1mZ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#@markdown ### Basic settings \n",
        "#@markdown We're still figuring out default settings. Experiment and <a href=\"https://huggingface.co/datasets/multimodalart/latent-majesty-diffusion-settings\">share your settings with us</a>\n",
        "majesty.width =  256#@param{type: 'integer'}\n",
        "majesty.height =  256#@param{type: 'integer'}\n",
        "#@markdown The `latent_diffusion_guidance_scale` will determine how much the `latent_prompts` affect the image. Lower help with text interpretation, higher help with composition. Try values between 0-15. If you see too much text, lower it  \n",
        "majesty.latent_diffusion_guidance_scale = 12 #@param {type:\"number\"}\n",
        "#@markdown The `clamp_index` will determine how much of the `clip_prompts` affect the image, it is a linear scale that will decrease from the first to the second value. Try values between 3-1\n",
        "majesty.clamp_index = [2.4, 2.1] #@param{type: 'raw'}\n",
        "majesty.clip_guidance_scale =  16000#@param{type: 'integer'}\n",
        "majesty.how_many_batches = 1 #@param{type: 'integer'}\n",
        "majesty.aesthetic_loss_scale = 400 #@param{type: 'integer'}\n",
        "majesty.augment_cuts=True #@param{type:'boolean'}\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown  ### Init image settings\n",
        "#@markdown `init_image` requires the path of an image to use as init to the model\n",
        "majesty.init_image = None #@param{type: 'string'}\n",
        "if(majesty.init_image == '' or majesty.init_image == 'None'):\n",
        "  majesty.init_image = None\n",
        "#@markdown `starting_timestep`: How much noise do you want to add to your init image for it to then be difused by the model\n",
        "majesty.starting_timestep = 0.9 #@param{type: 'number'}\n",
        "#@markdown `init_mask` is a mask same width and height as the original image with the color black indicating where to inpaint\n",
        "majesty.init_mask = None #@param{type: 'string'}\n",
        "#@markdown `init_scale` controls how much the init image should influence the final result. Experiment with values around `1000`\n",
        "majesty.init_scale = 1000 #@param{type: 'integer'}\n",
        "majesty.init_brightness = 0.0 #@param{type: 'number'}\n",
        "#  @markdown How much extra noise to add to the init image, independently from skipping timesteps (use it also if you are upscaling)\n",
        "#majesty.init_noise = 0.57 #@param{type: 'number'}\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown ### Custom saved settings\n",
        "#@markdown If you choose custom saved settings, the settings set by the preset overrule some of your choices. You can still modify the settings not in the preset. <a href=\"https://github.com/multimodalart/majesty-diffusion/tree/main/latent_settings_library\">Check what each preset modifies here</a>\n",
        "majesty.custom_settings = 'path/to/settings.cfg' #@param{type:'string'}\n",
        "settings_library = 'None (use settings defined above)' #@param [\"None (use settings defined above)\", \"default\", \"defaults_v1_3\", \"dango233_princesses\", \"the_other_zippy_defaults\", \"makeitrad_defaults\"]\n",
        "if(settings_library != 'None (use settings defined above)'):\n",
        "  custom_settings = f'latent-majesty-diffusion-settings/{settings_library}.cfg'\n",
        "\n",
        "majesty.load_custom_settings()\n",
        "majesty.config_init_image()\n",
        "global progress\n",
        "progress = widgets.Image(layout = widgets.Layout(max_width = \"400px\",max_height = \"512px\"))\n",
        "display.display(progress)\n",
        "for n in trange(how_many_batches, desc=\"Sampling\"):\n",
        "  print(f\"Sampling images {n+1}/{how_many_batches}\")\n",
        "  majesty.prompts = clip_prompts\n",
        "  majesty.opt.prompt = latent_prompts\n",
        "  majesty.opt.uc = latent_negatives\n",
        "  majesty.set_custom_schedules()\n",
        "  majesty.config_clip_guidance()\n",
        "  majesty.config_output_size()\n",
        "  majesty.config_options()\n",
        "  majesty.opt.n_iter = 1 #Old way for batching, avoid touching\n",
        "  majesty.opt.n_samples =  1 #How many implaes in parallel. Breaks upscaling\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  majesty.generate_video = False\n",
        "  majesty.do_run()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cvUzcO9FeMT"
      },
      "source": [
        "### Save your own settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LGLUCX_UGqka"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@markdown ### Save current settings\n",
        "#@markdown If you would like to save your current settings, uncheck `skip_saving` and run this cell. You will get a `custom_settings.cfg` file you can reuse and share. If you like your results, <a href=\"https://huggingface.co/datasets/multimodalart/latent-majesty-diffusion-settings\">share your settings with us on the settings library</a>\n",
        "skip_saving = True #@param{type:'boolean'}\n",
        "if(not skip_saving):\n",
        "  data = majesty.generate_settings_file(add_prompts=False, add_dimensions=True)\n",
        "  text_file = open(\"custom_settings.cfg\", \"w\")\n",
        "  text_file.write(data)\n",
        "  text_file.close()\n",
        "  from google.colab import files\n",
        "  files.download('custom_settings.cfg')\n",
        "  print(\"Downloaded as custom_settings.cfg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzd-2mVMWHV0"
      },
      "source": [
        "### Biases acknowledgment\n",
        "Despite how impressive being able to turn text into image is, beware to the fact that this model may output content that reinforces or exarcbates societal biases. According to the <a href='https://arxiv.org/abs/2112.10752' target='_blank'>Latent Diffusion paper</a>:<i> \\\"Deep learning modules tend to reproduce or exacerbate biases that are already present in the data\\\"</i>. \n",
        "\n",
        "The model was trained on an unfiltered version the LAION-400M dataset, which scrapped non-curated image-text-pairs from the internet (the exception being the the removal of illegal content) and is meant to be used for research purposes, such as this one. <a href='https://laion.ai/laion-400-open-dataset/' target='_blank'>You can read more on LAION's website</a>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xEVSOJ4f0B21",
        "VpR9JhyCu5iq",
        "N_Di3xFSXGWe",
        "xEVSOJ4f0B21",
        "WOAs3ZvLlktt"
      ],
      "machine_shape": "hm",
      "name": "Latent Majesty Diffusion v1.6",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
